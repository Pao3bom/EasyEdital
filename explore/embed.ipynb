{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/paoebom/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL = True\n",
    "\n",
    "with open(\"../pre/global/not_prune.csv\", \"r\") as f:\n",
    "    not_prune = list(csv.reader(f, delimiter=\",\"))[0]\n",
    "\n",
    "directory = \"../data\"\n",
    "\n",
    "if TOTAL:\n",
    "\n",
    "    data_list = not_prune\n",
    "else:\n",
    "\n",
    "    data_list = random.sample(not_prune, k=10000)\n",
    "    \"\"\" directory = \"../pre/sample\"\n",
    "\n",
    "    data_list = os.listdir(directory)\n",
    "    data_list = [data for data in data_list if data in not_prune] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SENTENCE_TRANSFORMERS = True\n",
    "\n",
    "if USE_SENTENCE_TRANSFORMERS:\n",
    "    #model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device='cuda')\n",
    "    model = SentenceTransformer('ulysses-camara/legal-bert-pt-br', device='cuda')\n",
    "\n",
    "else:\n",
    "    model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" with open(\"../pre/zipf/stopwords-0.10-0.50.csv\", \"r\") as f:\n",
    "    context_stopwords = csv.reader(f)\n",
    "    context_stopwords = list(context_stopwords)\n",
    "    context_stopwords = [words[0] for words in context_stopwords] \"\"\"\n",
    "\n",
    "doc_embeddings = {}\n",
    "# stop_words = set(stopwords.words('portuguese') + context_stopwords)\n",
    "\n",
    "if USE_SENTENCE_TRANSFORMERS:\n",
    "    for file in tqdm(data_list, total=len(data_list)):\n",
    "        with open(f'{directory}/{file}', 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            \"\"\" text = re.sub(r'[^\\w0-9- ]+', '', text, flags=re.UNICODE)\n",
    "            text = [x.strip() for x in text.split() if len(x) > 0 and x not in stop_words]\n",
    "            text = ' '.join(text) \"\"\"\n",
    "\n",
    "            doc_embeddings[file] = list(map(float, (model.encode(text, device='cuda'))))\n",
    "\n",
    "else:\n",
    "    for file in tqdm(data_list, total=len(data_list)):\n",
    "        with open(f'{directory}/{file}', 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            \"\"\" text = re.sub(r'[^\\w0-9- ]+', '', text, flags=re.UNICODE)\n",
    "            text = [x.strip() for x in text.split() if len(x) > 0 and x not in stop_words]\n",
    "            text = ' '.join(text) \"\"\"\n",
    "\n",
    "            input_ids = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outs = model(input_ids)\n",
    "                encoded = outs[0][0, 1:-1]\n",
    "\n",
    "            mean_pool_encoded = np.array(encoded).mean(axis=0)\n",
    "            doc_embeddings[file] = list(map(float, mean_pool_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TOTAL:\n",
    "    with open(\"../pre/global/full_embed.json\", 'w') as f: json.dump(doc_embeddings, f)\n",
    "else:\n",
    "    with open(\"../pre/global/partial_embed_no_clean.json\", 'w') as f: json.dump(doc_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_3372/2649129478.py:17: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  \"\"\" text = re.sub(r'[^\\w0-9- ]+', '', text, flags=re.UNICODE)\n"
     ]
    }
   ],
   "source": [
    "def get_similar_docs(document_file:str, embed_json_path:str=\"../pre/global/full_embed.json\", max_recs:int|str=5, simplified:bool=False) -> list|int:\n",
    "\n",
    "    if max_recs is not int and max_recs != 'MAX':\n",
    "        print(f'Invalid value of maximum recommendations! It must  be either \\'MAX\\' or an interger. Current value = {max_recs}')\n",
    "        return -1\n",
    "    \n",
    "    with open(embed_json_path, 'r') as f:\n",
    "        embed_dict = json.load(f)\n",
    "\n",
    "    if simplified:\n",
    "        doc_embeds = np.array(embed_dict[document_file])\n",
    "    else: \n",
    "        # stop_words = set(stopwords.words('portuguese'))\n",
    "        with open(document_file, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            \"\"\" text = re.sub(r'[^\\w0-9- ]+', '', text, flags=re.UNICODE)\n",
    "            text = [x.strip() for x in text.split() if len(x) > 0 and x not in stop_words]\n",
    "            text = ' '.join(text) \"\"\"\n",
    "\n",
    "            doc_embeds = np.array(model.encode(text, device='cuda'))\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for file in embed_dict.keys():\n",
    "        embed_value = np.array(embed_dict[file])\n",
    "        similarity = cosine_similarity(doc_embeds.reshape(1, -1), embed_value.reshape(1, -1))\n",
    "        results.append((file, similarity[0][0]))\n",
    "\n",
    "\n",
    "    ordered_results = sorted(results, reverse=True, key=lambda x:x[1])\n",
    "\n",
    "    if simplified: ordered_results = ordered_results[1:]\n",
    "\n",
    "    if max_recs is int and len(ordered_results) >= max_recs+1:\n",
    "        return ordered_results[: max_recs]\n",
    "\n",
    "    return ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [21:58<00:00, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maxima geral de similaridade entre documentos: 0.8923135436801104\n",
      "Minima geral de similaridade entre documentos: 0.027243755740902462\n",
      "Média geral de similaridade entre documentos: 0.4762353500585687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_num = 100\n",
    "test_docs = random.sample(data_list, k=sample_num)\n",
    "\n",
    "max_total = 0\n",
    "min_total = 0\n",
    "total = 0\n",
    "for doc in tqdm(test_docs, total=len(test_docs)):\n",
    "    #print(f'Documento em questão: {doc}')\n",
    "    results = get_similar_docs(doc, max_recs='MAX', simplified=True, embed_json_path=\"../pre/global/full_embed.json\")\n",
    "    #print(results[:5])\n",
    "    average = sum([value[1] for value in results])/len(results)\n",
    "    max_total += results[0][1]\n",
    "    min_total += results[-1][1]\n",
    "    total += average\n",
    "    #print(f'Maximum Similarity = {results[0]}')\n",
    "    #print(f'Minimum Similarity = {results[-1]}')\n",
    "    #print(f'Average Similarity = {average}')\n",
    "    #print()\n",
    "\n",
    "print()\n",
    "print(f'Maxima geral de similaridade entre documentos: {max_total/sample_num}')\n",
    "print(f'Minima geral de similaridade entre documentos: {min_total/sample_num}')\n",
    "print(f'Média geral de similaridade entre documentos: {total/sample_num}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
